{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNOwWgYWpOuCXiQrVujcKHr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"c2yA0kVnPvoE"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.metrics import confusion_matrix, f1_score"]},{"cell_type":"code","source":["zip_file = keras.utils.get_file(\n","    fname=\"cora.tgz\",\n","    origin=\"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\",\n","    extract=True,\n",")\n","data_dir = os.path.join(os.path.dirname(zip_file), \"cora\")"],"metadata":{"id":"c1rrLz-kP4tU","executionInfo":{"status":"ok","timestamp":1668517362585,"user_tz":-60,"elapsed":1177,"user":{"displayName":"Saskia de Wit","userId":"06599503112206246776"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"df3239e1-240d-4628-edd7-b85dfa68084b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n","168052/168052 [==============================] - 0s 1us/step\n"]}]},{"cell_type":"code","source":["citations = pd.read_csv(\n","    os.path.join(data_dir, \"cora.cites\"),\n","    sep=\"\\t\",\n","    header=None,\n","    names=[\"target\", \"source\"],\n",")\n","print(\"Citations shape:\", citations.shape)\n","type(citations)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XYgz94RJP6m7","executionInfo":{"status":"ok","timestamp":1668517365609,"user_tz":-60,"elapsed":450,"user":{"displayName":"Saskia de Wit","userId":"06599503112206246776"}},"outputId":"8721d4fc-ce59-499c-a6bb-43e519275821"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Citations shape: (5429, 2)\n"]},{"output_type":"execute_result","data":{"text/plain":["pandas.core.frame.DataFrame"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n","papers = pd.read_csv(\n","    os.path.join(data_dir, \"cora.content\"), sep=\"\\t\", header=None, names=column_names,\n",")\n","print(\"Papers shape:\", papers.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MEKaq8e8P8XJ","executionInfo":{"status":"ok","timestamp":1668517370997,"user_tz":-60,"elapsed":891,"user":{"displayName":"Saskia de Wit","userId":"06599503112206246776"}},"outputId":"969f007c-7df0-4cd6-df7a-95f7c4499d95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Papers shape: (2708, 1435)\n"]}]},{"cell_type":"code","source":["class_values = sorted(papers[\"subject\"].unique())\n","class_idx = {name: id for id, name in enumerate(class_values)}\n","paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n","\n","papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n","citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n","citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n","papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])"],"metadata":{"id":"sA9gtNuQP-LZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data, test_data = [], []\n","\n","for _, group_data in papers.groupby(\"subject\"):\n","    # Select around 80% of the dataset for training.\n","    random_selection = np.random.rand(len(group_data.index)) <= 0.8\n","    train_data.append(group_data[random_selection])\n","    test_data.append(group_data[~random_selection])\n","\n","train_data = pd.concat(train_data).sample(frac=1)\n","test_data = pd.concat(test_data).sample(frac=1)\n","\n","print(\"Train data shape:\", train_data.shape)\n","print(\"Test data shape:\", test_data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DJJ7Q5A6-Vgi","executionInfo":{"status":"ok","timestamp":1668517381430,"user_tz":-60,"elapsed":309,"user":{"displayName":"Saskia de Wit","userId":"06599503112206246776"}},"outputId":"e89ec2c6-ccd6-452d-9a51-6b610d3cfe11"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train data shape: (2163, 1435)\n","Test data shape: (545, 1435)\n"]}]},{"cell_type":"code","source":["feature_cols = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)]\n","train_data_X = train_data[feature_cols]\n","train_data_y = train_data['subject']\n","\n","test_data_X = test_data[feature_cols]\n","test_data_y = test_data['subject']"],"metadata":{"id":"i390Qd4z-dij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to decide the assigned label:\n","# keep al links between the papers\n","# to predict the value for a node: use all links of which we have a node label\n","\n","# for all nodes in the test set:\n","#   find all incoming and outgoing links that are in the train set\n","#   decide the most common label and assign it"],"metadata":{"id":"eiIE4k1r-km_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["flatten_list = lambda y:[x for a in y for x in flatten_list(a)] if type(y) is list else [y]\n","y_pred = np.empty((len(test_data_X)))\n","\n","for i in range(0, len(test_data_X)):\n","  # select the node to predict and find all incoming and outgoing link from that node\n","  pred_node = test_data_X.iloc[i,0]\n","  all_links = flatten_list([(citations[citations['source'] == pred_node].iloc[:,0]).tolist(), (citations[citations['target'] == pred_node].iloc[:,1]).tolist()])\n","  trgt_labels = []\n","  # for every link from/to the selected node:\n","  for j in range(0, len(all_links)):\n","    trgt = all_links[j]\n","    # if the connected node is in the train set (so it has a label)\n","    if trgt in train_data_X.iloc[:,0].tolist():\n","      # add the label to trgt_labels\n","      trgt_labels.append(papers[papers['paper_id'] == trgt].iloc[0, -1])\n","  trgt_labels = np.array(trgt_labels)\n","  # if there are no labels in trgt_labels (so we cannot predict), make the first prediction -1\n","  if len(trgt_labels) == 0:\n","    y_pred[i] = -1\n","  else:\n","    # otherwise find the most frequent label and assign it\n","    y_pred[i] = np.argmax(np.bincount(trgt_labels.astype(int)))"],"metadata":{"id":"E1g-qEXdP_yG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# there are some nodes that could not be predicted, because they only contained links with other train nodes\n","# these are the nodes with label -1\n","# now do the prediction for these nodes based on the new information"],"metadata":{"id":"h0wVEEjS-P-z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(0, len(test_data_X)):\n","  # for every node with prediction -1\n","  if y_pred[i] == -1:\n","    pred_node = test_data_X.iloc[i,0]\n","    # find all incoming and outgoing links that are also in the test set\n","    all_links = flatten_list([(citations[citations['source'] == pred_node].iloc[:,0]).tolist(), (citations[citations['target'] == pred_node].iloc[:,1]).tolist()])\n","    trgt_labels = []\n","    for j in range(0, len(all_links)):\n","      trgt = all_links[j]\n","      trgt_labels.append(y_pred[j])\n","    trgt_labels = np.array(trgt_labels)\n","    if len(trgt_labels) == 0:\n","      y_pred[i] = -1\n","    else:\n","      y_pred[i] = np.argmax(np.bincount(trgt_labels.astype(int)))"],"metadata":{"id":"GB2U8QLTlUuv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["correct = 0\n","for i in range(0, len(y_pred)):\n","  if np.array(test_data_y)[i] == y_pred[i]:\n","    correct += 1"],"metadata":{"id":"-wAZB8jplfVy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Accuracy: \", correct / len(y_pred))\n","print(\" \")\n","print('Confucsion matrix: ')\n","print(confusion_matrix(np.array(test_data_y) , y_pred))\n","print(\" \")\n","print(\"F1 score: \", f1_score(np.array(test_data_y) , y_pred, average = 'macro'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FKpnm_pMoAeE","executionInfo":{"status":"ok","timestamp":1668517688343,"user_tz":-60,"elapsed":394,"user":{"displayName":"Saskia de Wit","userId":"06599503112206246776"}},"outputId":"c5b4fa76-eeff-4563-b21a-aadad284f61c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.8440366972477065\n"," \n","Confucsion matrix: \n","[[ 46   2   4   1   0   0   1]\n"," [  4  79   2   1   0   0   0]\n"," [  0   6 143  10   1   0   6]\n"," [  2   1   6  78   2   0   4]\n"," [  1   2   2   0  26   0   2]\n"," [  5   1   1   1   0  38   2]\n"," [  5   1   5   3   1   0  50]]\n"," \n","F1 score:  0.8365484673439608\n"]}]}]}