{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO30A3oo2AVMUnikX+ASwNj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gqVN5pWxq6hW"},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.metrics import confusion_matrix, f1_score"]},{"cell_type":"code","source":["zip_file = keras.utils.get_file(\n","    fname=\"webkb.tgz\",\n","    origin=\"https://linqs-data.soe.ucsc.edu/public/lbc/WebKB.tgz\",\n","    extract=True,\n",")\n","data_dir = os.path.join(os.path.dirname(zip_file), \"webkb\")"],"metadata":{"id":"3J6MDe6yq7ia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["citations_cornell = pd.read_csv(\n","    os.path.join(data_dir, \"cornell.cites\"),\n","    sep=\"\\t\",\n","    header=None,\n","    names = ['urls']\n",")\n","\n","citations_texas = pd.read_csv(\n","    os.path.join(data_dir, \"texas.cites\"),\n","    sep=\"\\t\",\n","    header=None,\n","    names = ['urls']\n",")\n","\n","citations_washington = pd.read_csv(\n","    os.path.join(data_dir, \"washington.cites\"),\n","    sep=\"\\t\",\n","    header=None,\n","    names = ['urls']\n",")\n","\n","citations_wisconsin = pd.read_csv(\n","    os.path.join(data_dir, \"wisconsin.cites\"),\n","    sep=\"\\t\",\n","    header=None,\n","    names = ['urls']\n",")\n","\n","citations = citations_cornell.append(citations_texas.append(citations_washington.append(citations_wisconsin,  ignore_index=True),  ignore_index=True),  ignore_index=True)"],"metadata":{"id":"rMF_C_GOq9Tc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["citations['urls'].str.split(\" \")\n","\n","citations = citations[\"urls\"].str.split(\" \", n = 1, expand = True)\n","citations.rename(columns = {0 : 'source', 1 : 'target'}, inplace = True)"],"metadata":{"id":"Y5AB6LlRq-5O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["column_names = [\"url\"] + [f\"term_{idx}\" for idx in range(1703)] + [\"subject\"]\n","urls_cornell = pd.read_csv(\n","    os.path.join(data_dir, \"cornell.content\"), sep = \"\\t\", header = None, names=column_names)\n","\n","urls_texas = pd.read_csv(\n","    os.path.join(data_dir, \"texas.content\"), sep = \"\\t\", header = None, names=column_names)\n","\n","urls_washington = pd.read_csv(\n","    os.path.join(data_dir, \"washington.content\"), sep = \"\\t\", header = None, names=column_names)\n","\n","urls_wisconsin = pd.read_csv(\n","    os.path.join(data_dir, \"wisconsin.content\"), sep = \"\\t\", header = None, names=column_names)\n","\n","urls = urls_cornell.append(urls_texas.append(urls_washington.append(urls_wisconsin,  ignore_index=True),  ignore_index=True),  ignore_index=True)"],"metadata":{"id":"IexgNTVHrA9i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_values = sorted(urls[\"subject\"].unique())\n","class_idx = {name: id for id, name in enumerate(class_values)}\n","url_idx = {name: idx for idx, name in enumerate(sorted(urls[\"url\"].unique()))}\n","\n","urls[\"url\"] = urls[\"url\"].apply(lambda name: url_idx[name])\n","citations[\"source\"] = citations[\"source\"].apply(lambda name: url_idx[name])\n","citations[\"target\"] = citations[\"target\"].apply(lambda name: url_idx[name])\n","urls[\"subject\"] = urls[\"subject\"].apply(lambda value: class_idx[value])"],"metadata":{"id":"mVSEm3cVrGmG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data, test_data = [], []\n","\n","for _, group_data in urls.groupby(\"subject\"):\n","    # Select around 80% of the dataset for training.\n","    random_selection = np.random.rand(len(group_data.index)) <= 0.8\n","    train_data.append(group_data[random_selection])\n","    test_data.append(group_data[~random_selection])\n","\n","train_data = pd.concat(train_data).sample(frac=1)\n","test_data = pd.concat(test_data).sample(frac=1)\n","\n","print(\"Train data shape:\", train_data.shape)\n","print(\"Test data shape:\", test_data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A53MvRtmrIMa","executionInfo":{"status":"ok","timestamp":1668519813559,"user_tz":-60,"elapsed":264,"user":{"displayName":"Saskia de Wit","userId":"06599503112206246776"}},"outputId":"c8b91843-89b5-47f7-9fd1-7b0ebc2d4d7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train data shape: (711, 1705)\n","Test data shape: (166, 1705)\n"]}]},{"cell_type":"code","source":["feature_cols = [\"url\"] + [f\"term_{idx}\" for idx in range(1703)]\n","train_data_X = train_data[feature_cols]\n","train_data_y = train_data['subject']\n","\n","test_data_X = test_data[feature_cols]\n","test_data_y = test_data['subject']"],"metadata":{"id":"B0vnr0DOrJ3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to decide the assigned label:\n","# keep al links between the papers\n","# to predict the value for a node: use all links of which we have a node label\n","\n","# for all nodes in the test set:\n","#   find all incoming and outgoing links that are in the train set\n","#   decide the most common label and assign it"],"metadata":{"id":"7DCOWI_JiIXh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["flatten_list = lambda y:[x for a in y for x in flatten_list(a)] if type(y) is list else [y]\n","y_pred = np.empty((len(test_data_X)))\n","\n","for i in range(0, len(test_data_X)):\n","  pred_node = test_data_X.iloc[i,0]\n","  all_links = flatten_list([(citations[citations['source'] == pred_node].iloc[:,1]).tolist(), (citations[citations['target'] == pred_node].iloc[:,0]).tolist()])\n","  trgt_labels = []\n","  for j in range(0, len(all_links)):\n","    trgt = all_links[j]\n","    if trgt in train_data_X.iloc[:,0].tolist():\n","      trgt_labels.append(urls[urls['url'] == trgt].iloc[0, -1])\n","  trgt_labels = np.array(trgt_labels)\n","  if len(trgt_labels) == 0:\n","    y_pred[i] = 10\n","  else:\n","    y_pred[i] = np.argmax(np.bincount(trgt_labels.astype(int)))"],"metadata":{"id":"kMXfZsUsrYnd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# there are some nodes that could not be predicted, because they only contained links with other train nodes\n","# these are the nodes with label -1\n","# now do the prediction for these nodes based on the new information"],"metadata":{"id":"Z4fb1dTIiJin"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(0, len(test_data_X)):\n","  if y_pred[i] == 10:\n","    pred_node = test_data_X.iloc[i,0]\n","    all_links = flatten_list([(citations[citations['source'] == pred_node].iloc[:,1]).tolist(), (citations[citations['target'] == pred_node].iloc[:,0]).tolist()])\n","    trgt_labels = []\n","    for j in range(0, len(all_links)):\n","      trgt = all_links[j]\n","      trgt_labels.append(y_pred[j])\n","    trgt_labels = np.array(trgt_labels)\n","    if len(trgt_labels) == 0:\n","      y_pred[i] = 10\n","    else:\n","      y_pred[i] = np.argmax(np.bincount(trgt_labels.astype(int)))"],"metadata":{"id":"vpejbqXzrpTx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["correct = 0\n","for i in range(0, len(y_pred)):\n","  if np.array(test_data_y)[i] == y_pred[i]:\n","    correct += 1"],"metadata":{"id":"YcwP11IKrrvn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Accuracy: \", correct / len(y_pred))\n","print(\" \")\n","print('Confucsion matrix: ')\n","print(confusion_matrix(np.array(test_data_y) , y_pred))\n","print(\" \")\n","print(\"F1 score: \", f1_score(np.array(test_data_y) , y_pred, average = 'macro'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U94I-o3QrtmM","executionInfo":{"status":"ok","timestamp":1668519881482,"user_tz":-60,"elapsed":24,"user":{"displayName":"Saskia de Wit","userId":"06599503112206246776"}},"outputId":"7101ca62-48e2-402b-fb69-5833acb788f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.12650602409638553\n"," \n","Confucsion matrix: \n","[[14 11  2  0 13]\n"," [13  1  4  0  9]\n"," [ 6  7  2  2  3]\n"," [ 7  1  1  1  0]\n"," [52  9  5  0  3]]\n"," \n","F1 score:  0.11623687612165479\n"]}]}]}